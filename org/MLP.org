#+TITLE: CIFAR-10 Image Classification Task 
#+AUTHOR: Simon Beaumont
#+EMAIL: datalligator@icloud.com
#+SETUPFILE: org-source-setup.org

* GDL2e Chapter 2 Examples

** Some basic utility functions
*** Flux and required packages.
#+begin_src jupyter-julia
using Flux, Metal, MLUtils, OneHotArrays, Statistics, ProgressMeter
#+end_src

#+RESULTS:

*** Get the CIFAR10 image data
#+begin_src jupyter-julia
using MLDatasets: CIFAR10

# Get the 60,000 32x32 pixel color image data
function cifar10_data() 
    # Split the 60,000 images into training and testing observations
    # and make sure we have normalized Float32 pixel data.
    (CIFAR10(Tx=Float32, split=:train), CIFAR10(Tx=Float32, split=:test))
end
#+end_src

#+RESULTS:
: cifar10_data (generic function with 1 method)

*** Create one-hot encoding of targets/labels
#+begin_src jupyter-julia 
function onehotlabels(data ::CIFAR10)
    onehotbatch(data.targets, range(extrema(data.targets)...))
end
#+end_src

#+RESULTS:
: onehotlabels (generic function with 1 method)

*** Dataloader
#+begin_src jupyter-julia
function loader(data::CIFAR10; batchsize)
    x = data.features
    y = onehotlabels(data)
    Flux.DataLoader((x, y); batchsize, shuffle=true)
end
#+end_src

#+RESULTS:
: loader (generic function with 1 method)

*** Hyperparameters
We provide a set of training /hyperparameters/ but prefer to call these training
parameters.
#+begin_src jupyter-julia
struct TrainingParams
    batchsize :: Int
    epochs    :: Int
    learnrate :: Float64
end

function trainparams(;batchsize::Int, epochs::Int, learnrate::Float64)
    TrainingParams(batchsize, epochs, learnrate)
end
#+end_src

#+RESULTS:
: trainparams (generic function with 1 method)

*** Compute accuracy of predictions
#+begin_comment
N.B. Me sure to this on the cpu so that all the data fits in one go.
#+end_comment
#+begin_src jupyter-julia
function accuracy(m, data::CIFAR10, args::TrainingParams)
    (x, y) = only(loader(data; batchsize=(length(data))))
    y_hat = m(x)
    iscorrect = Flux.onecold(y_hat) .== Flux.onecold(y)
    round(100 * mean(iscorrect); digits=2)
end
#+end_src

#+RESULTS:
: accuracy (generic function with 1 method)

** Let's put those functions to work to train the model network.
#+begin_comment
Note: Thuse seemed relevant but not used as indicated in most of the
Flux examples I've seen: [[https://fluxml.ai/Flux.jl/stable/training/reference/#Optimisers.update!][update]] [[https://fluxml.ai/Flux.jl/stable/training/zygote/#Zygote.withgradient-Tuple{Any,%20Vararg{Any}}][withgradient]]. YMMV
#+end_comment
#+begin_src jupyter-julia
# constructor with keyword args
function trainwith(model, train_data::CIFAR10, args::TrainingParams; device)
    @info "trainwith" args
    # model
    md = device(model)
    # loader
    train_loader = loader(train_data, batchsize=args.batchsize)
    # optimizer state with training rate
    opt_state = Flux.setup(Adam(args.learnrate), md)
    
    losses = [] # keep track of loss at each epoch
    
    @showprogress for epoch in 1:args.epochs
        for (x_batch, y_batch) in train_loader
            # device transfer if required
            x, y = device(x_batch), device(y_batch)
            # compute loss and gradients
            l, gs = Flux.withgradient(m -> Flux.crossentropy(m(x), y), md)
            # update model parameters
            Flux.update!(opt_state, md, gs[1]) # see: withgradient
            # accumulate losses for logging
            push!(losses, l)
        end
    end
    return (md, losses, length(train_loader))
end
#+end_src

#+RESULTS:
: trainwith (generic function with 1 method)

** Main train and test
#+begin_src jupyter-julia
using Plots

function trainandtest(model, tparam::TrainingParams; device=cpu)
    @info "Loading CIFAR10 data..."
    train, test = cifar10_data()

    @info "Training..."
    (trained, losses, n) = trainwith(model, train, tparam, device=device)

    @info "Testing..."
    testm = cpu(trained)
    train_a = accuracy(testm, train, tparam)
    test_a = accuracy(testm, test, tparam)
    @info "Accuracy:" train_a test_a

    # output a plot of loss
    plot(losses; xaxis=(:log10, "iteration"),
         yaxis="loss", label="per batch")
    # mean loss for epoch
    plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),
          label="epoch mean", dpi=200)
end
#+end_src

#+RESULTS:
: trainandtest (generic function with 1 method)

** The simple model from GDL2e Chapter 2

   Note: [[https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.softmax][softmax]] must not be passed to layers like Dense, which accept
   an activation function, as activation is broadcasted; if you get
   errors dispatching softmax this might be the problem.
   
#+begin_src jupyter-julia
function simplemodel()
    Chain(MLUtils.flatten,
          Dense(32^2 * 3 => 200, relu),
          Dense(200 => 150, relu),
          Dense(150 => 10),
          softmax)
end
#+end_src

#+RESULTS:
: simplemodel (generic function with 1 method)

** Train and test the results of simple model
Note: Doing this on the GPU takes twice as long as the CPUs which
takes 1:17 mins on my laptop using 8 cores.
#+begin_src jupyter-julia :tangle no
trainandtest(simplemodel(),
             trainparams(batchsize=32,
                         epochs=10,
                         learnrate=5e-4),
             device=cpu)
#+end_src

#+RESULTS:
:RESULTS:
: [ Info: Loading CIFAR10 data...
: [ Info: Training...
: ┌ Info: trainwith
: └   args = TrainingParams(32, 10, 0.0005)
: Progress: 100%|█████████████████████████████████████████| Time: 0:02:22
: 
: [ Info: Testing...
: ┌ Info: Accuracy:
: │   train_a = 53.69
: └   test_a = 50.15
[[file:./.ob-jupyter/80a7a898153115b5384cfe5b345e8540ff557f35.svg]]
:END:

** The convolutional model (CNN)
Following the batch, activation, dropout (BAD) method.  NB: a kernel
size of 3 in /Keras/ conv2d is (3,3) in the more generic /Flux/ Conv.

#+begin_src jupyter-julia
function cnnmodel()
    Chain(
        # 1
        Conv((3,3), 3 => 32; pad=SamePad(), stride=1),
        BatchNorm(32, rrelu),
        # 2
        Conv((3,3), 32 => 32; pad=SamePad(), stride=2),
        BatchNorm(32, rrelu),
        # 3
        Conv((3,3), 32 => 64; pad=SamePad(), stride=1),
        BatchNorm(64, rrelu),
        # 4
        Conv((3,3), 64 => 64, pad = SamePad(), stride=2),
        BatchNorm(64, rrelu),
        # 5
        MLUtils.flatten,
        Dense(4096 => 128),
        BatchNorm(128, rrelu),
        Dropout(0.5),
        # 6
        Dense(128 => 10),
        softmax
    )
end
#+end_src

#+RESULTS:
: cnnmodel (generic function with 1 method)
** Train and test the convolutional model
Note: Doing this on GPU fails miserably with some scalar indexing bug
in one of the layers. This was /Metal/ driver so probably not worth
investigating at this point. I'll update packages and try again
sometime (18-Dec-23) On my laptop using 8 cores this takes about 30
mins to train.

#+begin_src jupyter-julia :tangle no
trainandtest(cnnmodel(),
             trainparams(batchsize=32,
                         epochs=10,
                         learnrate=5e-4),
             device=cpu)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
[ Info: Loading CIFAR10 data...
[ Info: Training...
┌ Info: trainwith
└   args = TrainingParams(32, 10, 0.0005)
Progress: 100%|█████████████████████████████████████████| Time: 0:34:11

[ Info: Testing...┌ Info: Accuracy:
│   train_a = 83.41
└   test_a = 72.32
#+end_example
[[file:./.ob-jupyter/6a6638c82dacd7b86cb375753a1a75ad8b3e67b0.svg]]
:END:






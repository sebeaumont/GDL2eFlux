#+TITLE: Multilayer Perceptron XOR 
#+AUTHOR: Simon Beaumont
#+EMAIL: datalligator@icloud.com

#+BIBLIOGRAPHY: ../citations.bib
#+STARTUP: inlineimages overview latexpreview indent entitiespretty 
#+PROPERTY: header-args:jupyter-julia :session "jl" :async yes :kernel gdl2eflux-1.9

Example MLP from the Flux Quick Start documentation, hopefully we've already got
everything we need installed into our project package and a suitable Jupyter
kernel setup to use this environment. If not see [[../INSTALL.org][how to set up environment]].

[[[[https://fluxml.ai/Flux.jl/stable/models/quickstart/#man-quickstart]]]]

* A Neural Network in One Minute
If you have used neural networks before, then this simple example
might be helpful for seeing how the major parts of Flux work together.

#+BEGIN_SRC jupyter-julia

using Flux, Metal, Statistics, ProgressMeter

# Data  for XOR problem
# 2x1000 random Float32 matrix
noisy = rand(Float32, 2, 1000)
# 1000 element vector of Bool
truth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]

# MLP with one hidden layer of size 3
model = Chain(
    Dense(2 => 3, tanh),
    BatchNorm(3),
    Dense(3 => 2),
    softmax) |> gpu

# The model encapsulates parameters, randomly initialised. Its initial output is:
out1 = model(noisy |> gpu) |> cpu # 2×1000 Matrix{Float32}

# To train the model, we use batches of 64 samples, and one-hot encoding:
target = Flux.onehotbatch(truth, [true, false]) # 2×1000 OneHotMatrix
loader = Flux.DataLoader((noisy, target) |> gpu, batchsize=64, shuffle=true);
# 16-element DataLoader with first element: (2×64 Matrix{Float32}, 2×64 OneHotMatrix)

optim = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc.

# Training loop, using the whole data set 1000 times:
losses = []
@showprogress for epoch in 1:1_000
    for (x, y) in loader
        loss, grads = Flux.withgradient(model) do m
            # Evaluate model and loss inside gradient context:
            y_hat = m(x)
            Flux.crossentropy(y_hat, y)
        end
        Flux.update!(optim, model, grads[1])
        push!(losses, loss)  # logging, outside gradient context
    end
end

optim # parameters, momenta and output have all changed
out2 = model(noisy |> gpu) |> cpu  # first row is prob. of true, second row p(false)

mean((out2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!
#+END_SRC

#+RESULTS:
:RESULTS:
: Progress: 100%|█████████████████████████████████████████| Time: 0:03:54
: 
: 0.969
:END:

#+BEGIN_SRC jupyter-julia
using Plots

p_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title="True classification", legend=false)
p_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=out1[1,:], title="Untrained network", label="", clims=(0,1))
p_done = scatter(noisy[1,:], noisy[2,:], zcolor=out2[1,:], title="Trained network", legend=false)

plot(p_true, p_raw, p_done, layout=(1,3), size=(750,250))
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/7c76ba4fe3371ef37532cc763c46b2096e323bab.svg]]

** Loss during training 
#+BEGIN_SRC jupyter-julia
plot(losses; xaxis=(:log10, "iteration"), yaxis="loss", label="per batch")
n = length(loader)
plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)), label="epoch mean", dpi=200)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/938138576f0975340cf21df4d4f16fe5183eaa99.svg]]

This XOR ("exclusive or") problem is a variant of the famous one which
drove Minsky and Papert to invent deep neural networks in 1969. For
small values of "deep" – this has one hidden layer, while earlier
perceptrons had none. (What they call a hidden layer, Flux calls the
output of the first layer, model[1](noisy).)


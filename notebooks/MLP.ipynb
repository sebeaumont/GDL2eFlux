{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDL2e Chapter 2 Examples\n",
    "\n",
    "## Some basic utility functions\n",
    "\n",
    "### Flux and required packages."
   ],
   "id": "dfd65236-56da-4ea7-bf88-8f0b4fd82f92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Metal, MLUtils, OneHotArrays, Statistics, ProgressMeter\n"
   ],
   "id": "339561a8-9871-4fd2-9574-a53680647a6a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the CIFAR10 image data"
   ],
   "id": "37e6ec69-c733-42dd-8134-4c9adac63f8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets: CIFAR10\n",
    "\n",
    "# Get the 60,000 32x32 pixel color image data\n",
    "function cifar10_data() \n",
    "    # Split the 60,000 images into training and testing observations\n",
    "    # and make sure we have normalized Float32 pixel data.\n",
    "    (CIFAR10(Tx=Float32, split=:train), CIFAR10(Tx=Float32, split=:test))\n",
    "end\n"
   ],
   "id": "ab030ef6-82e4-4bcf-81aa-d082264dbdf6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one-hot encoding of targets/labels"
   ],
   "id": "f7179c64-7c0c-42a2-9cb9-a90562a19363"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function onehotlabels(data ::CIFAR10)\n",
    "    onehotbatch(data.targets, range(extrema(data.targets)...))\n",
    "end\n"
   ],
   "id": "6f9386a7-67fc-49e4-80be-1cbe3142321b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ],
   "id": "d3e0e211-f76a-43e0-a135-bba3d735848c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loader(data::CIFAR10; batchsize)\n",
    "    x = data.features\n",
    "    y = onehotlabels(data)\n",
    "    Flux.DataLoader((x, y); batchsize, shuffle=true)\n",
    "end\n"
   ],
   "id": "916374e5-c63c-4854-99a5-975afb1e80b5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "We provide a set of training *hyperparameters* but prefer to call these\n",
    "training parameters."
   ],
   "id": "60cc3bb7-f829-4e4d-8fc4-f939c09aba9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TrainingParams\n",
    "    batchsize :: Int\n",
    "    epochs    :: Int\n",
    "    learnrate :: Float64\n",
    "end\n",
    "\n",
    "function trainparams(;batchsize::Int, epochs::Int, learnrate::Float64)\n",
    "    TrainingParams(batchsize, epochs, learnrate)\n",
    "end\n"
   ],
   "id": "901a206a-8628-4e1e-9e3f-9f8b573427bb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy of predictions"
   ],
   "id": "49cb6070-1bfd-453a-88f5-406612389e14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(m, data::CIFAR10, args::TrainingParams)\n",
    "    (x, y) = only(loader(data; batchsize=(length(data))))\n",
    "    y_hat = m(x)\n",
    "    iscorrect = Flux.onecold(y_hat) .== Flux.onecold(y)\n",
    "    round(100 * mean(iscorrect); digits=2)\n",
    "end\n"
   ],
   "id": "69c86f7e-d22c-4742-a425-a680821acd4d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put those functions to work to train the model network."
   ],
   "id": "0b69e4cc-ae1b-46ec-89cb-d9390e86be85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructor with keyword args\n",
    "function trainwith(model, train_data::CIFAR10, args::TrainingParams; device)\n",
    "    @info \"trainwith\" args\n",
    "    # model\n",
    "    md = device(model)\n",
    "    # loader\n",
    "    train_loader = loader(train_data, batchsize=args.batchsize)\n",
    "    # optimizer state with training rate\n",
    "    opt_state = Flux.setup(Adam(args.learnrate), md)\n",
    "\n",
    "    losses = [] # keep track of loss at each epoch\n",
    "\n",
    "    @showprogress for epoch in 1:args.epochs\n",
    "        for (x_batch, y_batch) in train_loader\n",
    "            # device transfer if required\n",
    "            x, y = device(x_batch), device(y_batch)\n",
    "            # compute loss and gradients\n",
    "            l, gs = Flux.withgradient(m -> Flux.crossentropy(m(x), y), md)\n",
    "            # update model parameters\n",
    "            Flux.update!(opt_state, md, gs[1]) # see: withgradient\n",
    "            # accumulate losses for logging\n",
    "            push!(losses, l)\n",
    "        end\n",
    "    end\n",
    "    return (md, losses, length(train_loader))\n",
    "end\n"
   ],
   "id": "2dcfa5f4-09bb-47a8-b03a-6dcd509f0877"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main train and test"
   ],
   "id": "7ac89483-29bd-410f-af4e-6d44b2d86f42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "function trainandtest(model, tparam::TrainingParams; device=cpu)\n",
    "    @info \"Loading CIFAR10 data...\"\n",
    "    train, test = cifar10_data()\n",
    "\n",
    "    @info \"Training...\"\n",
    "    (trained, losses, n) = trainwith(model, train, tparam, device=device)\n",
    "\n",
    "    @info \"Testing...\"\n",
    "    testm = cpu(trained)\n",
    "    train_a = accuracy(testm, train, tparam)\n",
    "    test_a = accuracy(testm, test, tparam)\n",
    "    @info \"Accuracy:\" train_a test_a\n",
    "\n",
    "    # output a plot of loss\n",
    "    plot(losses; xaxis=(:log10, \"iteration\"),\n",
    "         yaxis=\"loss\", label=\"per batch\")\n",
    "    # mean loss for epoch\n",
    "    plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n",
    "          label=\"epoch mean\", dpi=200)\n",
    "end\n"
   ],
   "id": "b18aeb64-6921-4fc0-b873-c206a4378aec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simple model from GDL2e Chapter 2\n",
    "\n",
    "Note:\n",
    "[softmax](https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.softmax)\n",
    "must not be passed to layers like Dense, which accept an activation\n",
    "function, as activation is broadcasted; if you get errors dispatching\n",
    "softmax this might be the problem."
   ],
   "id": "64fd4a6b-0a93-4db6-9423-1cedc3f6ea5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simplemodel()\n",
    "    Chain(MLUtils.flatten,\n",
    "          Dense(32^2 * 3 => 200, relu),\n",
    "          Dense(200 => 150, relu),\n",
    "          Dense(150 => 10),\n",
    "          softmax)\n",
    "end\n"
   ],
   "id": "d65bfd4e-363b-4c8b-9c4c-65337dec9f66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the results of simple model\n",
    "\n",
    "Note: Doing this on the GPU takes twice as long as the CPUs which takes\n",
    "1:17 mins on my laptop using 8 cores."
   ],
   "id": "2ebcdd67-f776-4ebf-92e3-bdd5b65838c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tangle": "no"
   },
   "outputs": [],
   "source": [
    "trainandtest(simplemodel(),\n",
    "             trainparams(batchsize=32,\n",
    "                         epochs=10,\n",
    "                         learnrate=5e-4),\n",
    "             device=cpu)\n"
   ],
   "id": "5427506b-af63-4921-9b9e-9ff2e2bc971f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolutional model (CNN)\n",
    "\n",
    "Following the batch, activation, dropout (BAD) method. NB: a kernel size\n",
    "of 3 in *Keras* conv2d is (3,3) in the more generic *Flux* Conv."
   ],
   "id": "30b1bf28-ef07-4a54-94e5-cd5dc5a96865"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cnnmodel()\n",
    "    Chain(\n",
    "        # 1\n",
    "        Conv((3,3), 3 => 32; pad=SamePad(), stride=1),\n",
    "        BatchNorm(32, rrelu),\n",
    "        # 2\n",
    "        Conv((3,3), 32 => 32; pad=SamePad(), stride=2),\n",
    "        BatchNorm(32, rrelu),\n",
    "        # 3\n",
    "        Conv((3,3), 32 => 64; pad=SamePad(), stride=1),\n",
    "        BatchNorm(64, rrelu),\n",
    "        # 4\n",
    "        Conv((3,3), 64 => 64, pad = SamePad(), stride=2),\n",
    "        BatchNorm(64, rrelu),\n",
    "        # 5\n",
    "        MLUtils.flatten,\n",
    "        Dense(4096 => 128),\n",
    "        BatchNorm(128, rrelu),\n",
    "        Dropout(0.5),\n",
    "        # 6\n",
    "        Dense(128 => 10),\n",
    "        softmax\n",
    "    )\n",
    "end\n"
   ],
   "id": "5836f278-1777-44df-b68f-610abfc36d71"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the convolutional model\n",
    "\n",
    "Note: Doing this on GPU fails miserably with some scalar indexing bug in\n",
    "one of the layers. This was *Metal* driver so probably not worth\n",
    "investigating at this point. I'll update packages and try again sometime\n",
    "(18-Dec-23) On my laptop using 8 cores this takes about 30 mins to\n",
    "train."
   ],
   "id": "5d40c985-dc8f-4f6c-aa2a-126a29d5c156"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tangle": "no"
   },
   "outputs": [],
   "source": [
    "trainandtest(cnnmodel(),\n",
    "             trainparams(batchsize=32,\n",
    "                         epochs=10,\n",
    "                         learnrate=5e-4),\n",
    "             device=cpu)\n"
   ],
   "id": "d01f15d4-d3aa-4380-9f5f-f2c4b69ba9f3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

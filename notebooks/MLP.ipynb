{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDL2e Chapter 2 Examples\n",
    "\n",
    "## Some basic utility functions\n",
    "\n",
    "### Flux and required packages."
   ],
   "id": "6ac0135c-98e5-4afe-b93b-b59056b4e007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Metal, MLUtils, OneHotArrays, Statistics, ProgressMeter\n"
   ],
   "id": "29dc4811-b475-408b-a66b-f86fbf40a7b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the CIFAR10 image data"
   ],
   "id": "abcd066d-5beb-4a0d-92b6-e76001f746c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets: CIFAR10\n",
    "\n",
    "# Get the 60,000 32x32 pixel color image data\n",
    "function cifar10_data() \n",
    "    # Split the 60,000 images into training and testing observations\n",
    "    # and make sure we have normalized Float32 pixel data.\n",
    "    (CIFAR10(Tx=Float32, split=:train), CIFAR10(Tx=Float32, split=:test))\n",
    "end\n"
   ],
   "id": "db7845f7-8a4c-4bd6-bd9a-dacd50d74242"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one-hot encoding of targets/labels"
   ],
   "id": "f31da58f-2969-4488-9da3-56150b05da2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function onehotlabels(data ::CIFAR10)\n",
    "    onehotbatch(data.targets, range(extrema(data.targets)...))\n",
    "end\n"
   ],
   "id": "2aa8c784-ea1c-4f92-829a-042610c3a6bf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ],
   "id": "7cea8b5a-0482-45dc-af1a-673697c28090"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loader(data::CIFAR10; batchsize)\n",
    "    x = data.features\n",
    "    y = onehotlabels(data)\n",
    "    Flux.DataLoader((x, y); batchsize, shuffle=true)\n",
    "end\n"
   ],
   "id": "c5cbf8ad-af39-4e13-bf9a-3a1b604121e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "We provide a set of training *hyperparameters* but prefer to call these\n",
    "training parameters."
   ],
   "id": "fcc24102-5133-4fcf-ac0e-f1d3c8962e50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TrainingParams\n",
    "    batchsize :: Int\n",
    "    epochs    :: Int\n",
    "    learnrate :: Float64\n",
    "end\n",
    "\n",
    "function trainparams(;batchsize::Int, epochs::Int, learnrate::Float64)\n",
    "    TrainingParams(batchsize, epochs, learnrate)\n",
    "end\n"
   ],
   "id": "c24ccc1a-0438-43f6-8c47-8a57ab9c0899"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy of predictions"
   ],
   "id": "0bd177ab-27ab-4b59-8c71-f557ba278e0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(m, data::CIFAR10, args::TrainingParams)\n",
    "    (x, y) = only(loader(data; batchsize=(length(data))))\n",
    "    y_hat = m(x)\n",
    "    iscorrect = Flux.onecold(y_hat) .== Flux.onecold(y)\n",
    "    round(100 * mean(iscorrect); digits=2)\n",
    "end\n"
   ],
   "id": "4deb0a02-3ef6-49be-94c4-a21022dd1e89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put those functions to work to train the model network."
   ],
   "id": "94c9616c-64a2-464b-9d66-ba9872bc12e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructor with keyword args\n",
    "function trainwith(model, train_data::CIFAR10, args::TrainingParams; device)\n",
    "    @info \"trainwith\" args\n",
    "    # model\n",
    "    md = device(model)\n",
    "    # loader\n",
    "    train_loader = loader(train_data, batchsize=args.batchsize)\n",
    "    # optimizer state with training rate\n",
    "    opt_state = Flux.setup(Adam(args.learnrate), md)\n",
    "\n",
    "    losses = [] # keep track of loss at each epoch\n",
    "\n",
    "    @showprogress for epoch in 1:args.epochs\n",
    "        for (x_batch, y_batch) in train_loader\n",
    "            # device transfer if required\n",
    "            x, y = device(x_batch), device(y_batch)\n",
    "            # compute loss and gradients\n",
    "            l, gs = Flux.withgradient(m -> Flux.crossentropy(m(x), y), md)\n",
    "            # update model parameters\n",
    "            Flux.update!(opt_state, md, gs[1]) # see: withgradient\n",
    "            # accumulate losses for logging\n",
    "            push!(losses, l)\n",
    "        end\n",
    "    end\n",
    "    return (md, losses, length(train_loader))\n",
    "end\n"
   ],
   "id": "186bcecb-6bfa-4715-8805-cd344df403dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main train and test"
   ],
   "id": "4a62476c-5a2e-4dff-b0fe-f0ba42b30f45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "function trainandtest(model, tparam::TrainingParams; device=cpu)\n",
    "    @info \"Loading CIFAR10 data...\"\n",
    "    train, test = cifar10_data()\n",
    "\n",
    "    @info \"Training...\"\n",
    "    (trained, losses, n) = trainwith(model, train, tparam, device=device)\n",
    "\n",
    "    @info \"Testing...\"\n",
    "    testm = cpu(trained)\n",
    "    train_a = accuracy(testm, train, tparam)\n",
    "    test_a = accuracy(testm, test, tparam)\n",
    "    @info \"Accuracy:\" train_a test_a\n",
    "\n",
    "    # output a plot of loss\n",
    "    plot(losses; xaxis=(:log10, \"iteration\"),\n",
    "         yaxis=\"loss\", label=\"per batch\")\n",
    "    # mean loss for epoch\n",
    "    plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n",
    "          label=\"epoch mean\", dpi=200)\n",
    "end\n"
   ],
   "id": "4c1a537b-c9c3-4a97-abbe-fb31c26c4bb6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simple model from GDL2e Chapter 2\n",
    "\n",
    "Note:\n",
    "[softmax](https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.softmax)\n",
    "must not be passed to layers like Dense, which accept an activation\n",
    "function, as activation is broadcasted; if you get errors dispatching\n",
    "softmax this might be the problem."
   ],
   "id": "2fbe732a-7481-4bf3-bf90-3149e6dd7e5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simplemodel()\n",
    "    Chain(MLUtils.flatten,\n",
    "          Dense(32^2 * 3 => 200, relu),\n",
    "          Dense(200 => 150, relu),\n",
    "          Dense(150 => 10),\n",
    "          softmax)\n",
    "end\n"
   ],
   "id": "f70f9023-d6fd-490d-9a34-14bb5f998588"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the results of simple model\n",
    "\n",
    "Note: Doing this on the GPU takes twice as long as the CPUs which takes\n",
    "1:17 mins on my laptop using 8 cores."
   ],
   "id": "49d548a1-6a1f-4bc7-9a1b-1a5299631340"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tangle": "no"
   },
   "outputs": [],
   "source": [
    "trainandtest(simplemodel(),\n",
    "             trainparams(batchsize=32,\n",
    "                         epochs=10,\n",
    "                         learnrate=5e-4),\n",
    "             device=cpu)\n"
   ],
   "id": "621b9c3f-c4e3-44ba-a017-6127201a77ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolutional model (CNN)\n",
    "\n",
    "Following the batch, activation, dropout (BAD) method. NB: a kernel size\n",
    "of 3 in *Keras* conv2d is (3,3) in the more generic *Flux* Conv."
   ],
   "id": "5002a7a4-fc4f-45b3-9b70-4c16112cea26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cnnmodel()\n",
    "    Chain(\n",
    "        # 1\n",
    "        Conv((3,3), 3 => 32; pad=SamePad(), stride=1),\n",
    "        BatchNorm(32, rrelu),\n",
    "        # 2\n",
    "        Conv((3,3), 32 => 32; pad=SamePad(), stride=2),\n",
    "        BatchNorm(32, rrelu),\n",
    "        # 3\n",
    "        Conv((3,3), 32 => 64; pad=SamePad(), stride=1),\n",
    "        BatchNorm(64, rrelu),\n",
    "        # 4\n",
    "        Conv((3,3), 64 => 64, pad = SamePad(), stride=2),\n",
    "        BatchNorm(64, rrelu),\n",
    "        # 5\n",
    "        MLUtils.flatten,\n",
    "        Dense(4096 => 128),\n",
    "        BatchNorm(128, rrelu),\n",
    "        Dropout(0.5),\n",
    "        # 6\n",
    "        Dense(128 => 10),\n",
    "        softmax\n",
    "    )\n",
    "end\n"
   ],
   "id": "5dd0ace8-bbcd-4a70-879a-00d4237b170f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the convolutional model\n",
    "\n",
    "Note: Doing this on GPU fails miserably with some scalar indexing bug in\n",
    "one of the layers. This was *Metal* driver so probably not worth\n",
    "investigating at this point. I'll update packages and try again sometime\n",
    "(18-Dec-23) On my laptop using 8 cores this takes about 30 mins to\n",
    "train."
   ],
   "id": "246092c3-da9c-4264-8173-2a912b4a1c98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tangle": "no"
   },
   "outputs": [],
   "source": [
    "trainandtest(cnnmodel(),\n",
    "             trainparams(batchsize=32,\n",
    "                         epochs=10,\n",
    "                         learnrate=5e-4),\n",
    "             device=cpu)\n"
   ],
   "id": "154440ed-5201-40de-ba65-136c220a82ee"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDL2e Chapter 2 Examples\n",
    "\n",
    "## Some basic utility functions\n",
    "\n",
    "### Flux and required packages."
   ],
   "id": "a3a59e19-aff8-4b2f-a368-9fbdfcd3ad7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Metal, MLUtils, OneHotArrays, Statistics, ProgressMeter\n"
   ],
   "id": "e7c52414-cf9c-4c4c-b924-faaaa9936581"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the CIFAR10 image data"
   ],
   "id": "de6eba22-2765-487b-a56a-652112a2e8bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets: CIFAR10\n",
    "\n",
    "# Get the 60,000 32x32 pixel color image data\n",
    "function cifar10_data() \n",
    "    # Split the 60,000 images into training and testing observations\n",
    "    # and make sure we have normalized Float32 pixel data.\n",
    "    (CIFAR10(Tx=Float32, split=:train), CIFAR10(Tx=Float32, split=:test))\n",
    "end\n"
   ],
   "id": "0f96a690-c59f-4a44-b610-ca428e2b8fec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one-hot encoding of targets/labels"
   ],
   "id": "10eab148-3877-41eb-b561-f74205d9cfb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function onehotlabels(data ::CIFAR10)\n",
    "    onehotbatch(data.targets, range(extrema(data.targets)...))\n",
    "end\n"
   ],
   "id": "77a44237-a880-40ea-b510-59f090cc3b30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ],
   "id": "440b01d0-bc61-4749-b0ba-92a9d83856f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loader(data::CIFAR10; batchsize)\n",
    "    x = data.features\n",
    "    y = onehotlabels(data)\n",
    "    Flux.DataLoader((x, y); batchsize, shuffle=true)\n",
    "end\n"
   ],
   "id": "1d5960ec-da05-4338-bdf7-d143aedc387a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "We provide a set of training *hyperparameters* but prefer to call these\n",
    "training parameters."
   ],
   "id": "f83d2aa2-932a-4d04-8299-973e47ac20af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TrainingParams\n",
    "    batchsize :: Int\n",
    "    epochs    :: Int\n",
    "    learnrate :: Float64\n",
    "end\n",
    "\n",
    "function trainparams(;batchsize::Int, epochs::Int, learnrate::Float64)\n",
    "    TrainingParams(batchsize, epochs, learnrate)\n",
    "end\n"
   ],
   "id": "7c0d23f8-24d0-49e4-8010-dde1d1c79737"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy of predictions"
   ],
   "id": "1071e1a6-a3dd-426a-a90b-9fad4fe52507"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(m, data::CIFAR10, args::TrainingParams)\n",
    "    (x, y) = only(loader(data; batchsize=(length(data))))\n",
    "    y_hat = m(x)\n",
    "    iscorrect = Flux.onecold(y_hat) .== Flux.onecold(y)\n",
    "    round(100 * mean(iscorrect); digits=2)\n",
    "end\n"
   ],
   "id": "94c9321f-a83d-49a1-85c2-1405d570974b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put those functions to work to train the model network."
   ],
   "id": "389a701d-bc90-4b09-a8f6-136dcb8a365b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructor with keyword args\n",
    "function trainwith(model, train_data::CIFAR10, args::TrainingParams; device)\n",
    "    @info \"trainwith\" args\n",
    "    # model\n",
    "    md = device(model)\n",
    "    # loader\n",
    "    train_loader = loader(train_data, batchsize=args.batchsize)\n",
    "    # optimizer state with training rate\n",
    "    opt_state = Flux.setup(Adam(args.learnrate), md)\n",
    "\n",
    "    losses = [] # keep track of loss at each epoch\n",
    "\n",
    "    @showprogress for epoch in 1:args.epochs\n",
    "        for (x_batch, y_batch) in train_loader\n",
    "            # device transfer if required\n",
    "            x, y = device(x_batch), device(y_batch)\n",
    "            # compute loss and gradients\n",
    "            l, gs = Flux.withgradient(m -> Flux.crossentropy(m(x), y), md)\n",
    "            # update model parameters\n",
    "            Flux.update!(opt_state, md, gs[1]) # see: withgradient\n",
    "            # accumulate losses for logging\n",
    "            push!(losses, l)\n",
    "        end\n",
    "    end\n",
    "    return (md, losses, length(train_loader))\n",
    "end\n"
   ],
   "id": "bd193ec9-6279-4710-8915-31414c261b02"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main train and test"
   ],
   "id": "2a90c7e1-506e-4d06-ae08-c24c04348933"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "function trainandtest(model, tparam::TrainingParams; device=cpu)\n",
    "    @info \"Loading CIFAR10 data...\"\n",
    "    train, test = cifar10_data()\n",
    "\n",
    "    @info \"Training...\"\n",
    "    (trained, losses, n) = trainwith(model, train, tparam, device=device)\n",
    "\n",
    "    @info \"Testing...\"\n",
    "    testm = cpu(trained)\n",
    "    train_a = accuracy(testm, train, tparam)\n",
    "    test_a = accuracy(testm, test, tparam)\n",
    "    @info \"Accuracy:\" train_a test_a\n",
    "\n",
    "    # output a plot of loss\n",
    "    plot(losses; xaxis=(:log10, \"iteration\"),\n",
    "         yaxis=\"loss\", label=\"per batch\")\n",
    "    # mean loss for epoch\n",
    "    plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n",
    "          label=\"epoch mean\", dpi=200)\n",
    "end\n"
   ],
   "id": "65aac855-88a8-41b2-9d16-d61c07179b3d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simple model from GDL2e Chapter 2\n",
    "\n",
    "Note:\n",
    "[softmax](https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.softmax)\n",
    "must not be passed to layers like Dense, which accept an activation\n",
    "function, as activation is broadcasted; if you get errors dispatching\n",
    "softmax this might be the problem."
   ],
   "id": "7c64337c-959a-4774-94a6-96cc6a385ef6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simplemodel()\n",
    "    Chain(MLUtils.flatten,\n",
    "          Dense(32^2 * 3 => 200, relu),\n",
    "          Dense(200 => 150, relu),\n",
    "          Dense(150 => 10),\n",
    "          softmax)\n",
    "end\n"
   ],
   "id": "2df35e2f-d029-4622-913b-89f3a8c82a09"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the results of simple model\n",
    "\n",
    "Note: Doing this on the GPU takes twice as long as the CPUs which takes\n",
    "1:17 mins on my laptop using 8 cores."
   ],
   "id": "cf6e1ee9-2936-4924-901b-f434425b544f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tangle": "no"
   },
   "outputs": [],
   "source": [
    "trainandtest(simplemodel(),\n",
    "             trainparams(batchsize=32,\n",
    "                         epochs=10,\n",
    "                         learnrate=5e-4),\n",
    "             device=cpu)\n"
   ],
   "id": "866c4554-bc21-4f4d-baf2-21a8e96cddc0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolutional model (CNN)\n",
    "\n",
    "Following the batch, activation, dropout (BAD) method. NB: a kernel size\n",
    "of 3 in *Keras* conv2d is (3,3) in the more generic *Flux* Conv."
   ],
   "id": "82bce10b-b8ac-40a5-bfc6-48191131e201"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cnnmodel()\n",
    "    Chain(\n",
    "        # 1\n",
    "        Conv((3,3), 3 => 32; pad=SamePad(), stride=1),\n",
    "        BatchNorm(32, rrelu),\n",
    "        # 2\n",
    "        Conv((3,3), 32 => 32; pad=SamePad(), stride=2),\n",
    "        BatchNorm(32, rrelu),\n",
    "        # 3\n",
    "        Conv((3,3), 32 => 64; pad=SamePad(), stride=1),\n",
    "        BatchNorm(64, rrelu),\n",
    "        # 4\n",
    "        Conv((3,3), 64 => 64, pad = SamePad(), stride=2),\n",
    "        BatchNorm(64, rrelu),\n",
    "        # 5\n",
    "        MLUtils.flatten,\n",
    "        Dense(4096 => 128),\n",
    "        BatchNorm(128, rrelu),\n",
    "        Dropout(0.5),\n",
    "        # 6\n",
    "        Dense(128 => 10),\n",
    "        softmax\n",
    "    )\n",
    "end\n"
   ],
   "id": "1e1f2c13-bddb-47ac-9482-97d2c0ddc7c3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the convolutional model\n",
    "\n",
    "Note: Doing this on GPU fails miserably with some scalar indexing bug in\n",
    "one of the layers. This was *Metal* driver so probably not worth\n",
    "investigating at this point. I'll update packages and try again sometime\n",
    "(18-Dec-23) On my laptop using 8 cores this takes about 30 mins to\n",
    "train."
   ],
   "id": "f5457840-4111-4ac7-b2e6-f1dcc9afd3a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tangle": "no"
   },
   "outputs": [],
   "source": [
    "trainandtest(cnnmodel(),\n",
    "             trainparams(batchsize=32,\n",
    "                         epochs=10,\n",
    "                         learnrate=5e-4),\n",
    "             device=cpu)\n"
   ],
   "id": "5273a332-2a6f-47b9-a970-18cb05235362"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

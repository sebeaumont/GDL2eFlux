{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example MLP from the *Flux Quick Start documentation*.\n",
    "\n",
    "Hopefully we've already got everything we need installed into our\n",
    "project package and a suitable *Jupyter* kernel to use this environment.\n",
    "If not see [how to set up environment](../INSTALL.org).\n",
    "\n",
    "The real source of this *HelloANNWorld* is here and I merely reproduce\n",
    "it in org source form as a test of the setup:\n",
    "\n",
    "\\[\\[<https://fluxml.ai/Flux.jl/stable/models/quickstart/#man-quickstart>\\]\\]\n",
    "\n",
    "# A Neural Network in One Minute\n",
    "\n",
    "If you have used neural networks before, then this simple example might\n",
    "be helpful for seeing how the major parts of *Flux* work together."
   ],
   "id": "515ea337-0a0f-4b5d-aed4-4741fc626ba2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using Flux, Metal, Statistics, ProgressMeter\n",
    "\n",
    "# Data  for XOR problem\n",
    "# 2x1000 random Float32 matrix\n",
    "noisy = rand(Float32, 2, 1000)\n",
    "# 1000 element vector of Bool\n",
    "truth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]\n",
    "\n",
    "# MLP with one hidden layer of size 3\n",
    "model = Chain(\n",
    "    Dense(2 => 3, tanh),\n",
    "    BatchNorm(3),\n",
    "    Dense(3 => 2),\n",
    "    softmax) |> gpu\n",
    "\n",
    "# The model encapsulates parameters, randomly initialised. Its initial output is:\n",
    "out1 = model(noisy |> gpu) |> cpu # 2×1000 Matrix{Float32}\n",
    "\n",
    "# To train the model, we use batches of 64 samples, and one-hot encoding:\n",
    "target = Flux.onehotbatch(truth, [true, false]) # 2×1000 OneHotMatrix\n",
    "loader = Flux.DataLoader((noisy, target) |> gpu, batchsize=64, shuffle=true);\n",
    "# 16-element DataLoader with first element: (2×64 Matrix{Float32}, 2×64 OneHotMatrix)\n",
    "\n",
    "optim = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc.\n",
    "\n",
    "# Training loop, using the whole data set 1000 times:\n",
    "losses = []\n",
    "@showprogress for epoch in 1:1_000\n",
    "    for (x, y) in loader\n",
    "        loss, grads = Flux.withgradient(model) do m\n",
    "            # Evaluate model and loss inside gradient context:\n",
    "            y_hat = m(x)\n",
    "            Flux.crossentropy(y_hat, y)\n",
    "        end\n",
    "        Flux.update!(optim, model, grads[1])\n",
    "        push!(losses, loss)  # logging, outside gradient context\n",
    "    end\n",
    "end\n",
    "\n",
    "optim # parameters, momenta and output have all changed\n",
    "out2 = model(noisy |> gpu) |> cpu  # first row is prob. of true, second row p(false)\n",
    "\n",
    "mean((out2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!\n"
   ],
   "id": "94d16971-8487-4e73-bade-9b3163023c11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "p_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=\"True classification\", legend=false)\n",
    "p_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=out1[1,:], title=\"Untrained network\", label=\"\", clims=(0,1))\n",
    "p_done = scatter(noisy[1,:], noisy[2,:], zcolor=out2[1,:], title=\"Trained network\", legend=false)\n",
    "\n",
    "plot(p_true, p_raw, p_done, layout=(1,3), size=(750,250))\n"
   ],
   "id": "29b51cfc-2f43-4731-8ef8-0b34430b5af5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss during training"
   ],
   "id": "d3d5e9c6-ae5e-4ca2-aec0-a6c8aa0adcbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses; xaxis=(:log10, \"iteration\"), yaxis=\"loss\", label=\"per batch\")\n",
    "n = length(loader)\n",
    "plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)), label=\"epoch mean\", dpi=200)\n"
   ],
   "id": "884a1a1d-d4e8-448a-b012-31b8e989e481"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This XOR (\"exclusive or\") problem is a variant of the famous one which\n",
    "drove Minsky and Papert to invent deep neural networks in 1969. For\n",
    "small values of \"deep\" – this has one hidden layer, while earlier\n",
    "perceptrons had none. (What they call a hidden layer, Flux calls the\n",
    "output of the first layer, model\\[1\\](noisy).)"
   ],
   "id": "3064f12e-4bc8-483c-92e4-0650f17b82af"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
